{
    "_id": "674c4418902fb1d903e1c66f",
    "email": "chandrakasturi@gmail.com",
    "url": "https://www.youtube.com/watch?v=bX1Q1atain4",
    "transcribedtext": "all right folks today we're going to learn how to use l chain at first I'll talk about L chain from a conceptual perspective and then we are going to jump into the code and build a very simple chain where you create your prompt pass it to the model and get an output the first thing to talk about is why do we need to use l chain all the models that are out there like open AI Claude and Gemini they have their own API that you can easily call and get the output that you want but the problem with doing it that way is your application code becomes very coupled or very strongly tied to the model that you're using later on when there's a better model with more functionality it's going to be very difficult for you to to move to that to that model also when you're just relying on the API everything that goes either in the prompt generation phase or formatting the output that you receive from the model any custom code that you need to handle those you have to write it yourself there's no like helper function that can that can make your job significantly easy on the other hand if you are using Lang chain Lang chain has a much more modular approach and it gives you a lot of tools that you can use to build your application in a very flexible way later on if you want to switch to a new model all you have to do is swap out the model I think it's like one one or two lenss of code rather than changing your whole application logic alongside that Lang chain also gives you a lot of powerful tools that you can use uh whether whether when you're formatting The Prompt or formatting the output so for instance let's say your prompt is going to be uh the trans the transcription of YouTube video or maybe the prompt is going to be uh let's say the prompt is going to be a PDF file or a CSV file if you don't use Lang chain you have to write a lot of parsing logic to convert that PDF for YouTube video into the prompt whereas with Lang chain they have a lot of this modular tools already built that Mak parsing these very common data formats very easy so we're going to look at a lot of those down the road but uh for the purposes of this video we're going to just quickly create a chain called the open AI API and get an output from it it's only going to be a few lines of code but hopefully that's going to be a good starting point that we can uh build upon in later weeks where we're building much more complex application with Lang chain okay so with that said let's get started uh okay I'm going to just quickly do a few Imports all of these are going to be from the different Lang chain uh libraries so you're going to uh I have already installed them in my environment but uh if you're looking on uh how to install them just either drop a comment below for me or I should have a link in the description with all the dependencies okay all right so let's quickly uh write the Imports at [Music] first so we have the chat open AI so this is the this is the uh library or the Lang chain open openai is the library that is the wrapper around the openai API so that you don't have to understand how exactly the API works you can just work with it through the through the chat open AI class okay next import is our output parser uh you're going to see it when we write the code for it but when when we get the response from openai or any other model alongside your true answer you also get a lot of metadata so you can definitely write the code to parse it or you can use something like a string output parser which is going to do the paring for you and just give you the result you care about again it's going to be much clearer when we actually write the code after the Imports and the last thing we're going to import is the prompt uh so we're going to import the chat prompt template okay and chat prompt template is one of the most powerful classes within l chain um this gives you the ability to very dynamically create your prompt before you pass it to your model okay all right so the first the first uh the first part will be to create the model okay so we have our model and then we're going to just explicitly say what model it is going to be it's going to be L gbd4 model okay so with this one line of code you have your model uh ready to interact with okay that's how simple it is U you don't have to uh create uh a request object uh make the call to the to the chat uh to the to the openi API and all those things you can literally with one line have a model ready to use next we're going to create our prompts okay so we're going to have a system prompt and anything Dynamic that you want to change you want to put it in uh uh within that format it spring so there we go and then we're going to have our I'll I'll explain the difference between system and human template in a little bit this is also going to be a F string and we want the ability to switch the city so we're going to make that another Dynamic string okay so what's the difference between a system template and a human template system template uh this is something you just tell the model uh how to either behave act give your responses the tone to use uh this is not a part of your actual question or actual prompt the human template is where you're actually asking the question okay this totally optional you can always just have the human template but this uh by having it into two separate templates we can at least uh yeah we can show like a little bit more about how you can like use that Dynamic nature of the uh of the prompt generation okay and now now that we have the two formatted string uh the next step is to actually create the chat template okay so we're going to create it using uh what we had there the chat promp template from messages what from messages does is uh it lets you create one big prompt from multiple different messages okay so multiple different messages in the sense you can have this can be one message this is another message uh so that lets you sort of like chain the messages together and in the end you have this one big prompt that you can send to your model as we from messages so let's give it give it an array the array the first one's going to be our system message so you want to say what kind of message is it is which is a system message and then the templ SL itself uh and then you have the user or human message okay so again now you have a list of two messages the first one is your system message and the second one is your human message okay so at this point you have the model created here uh so if we go to our diagram you have your model created and then you have your prompt created as well which is just a prompt over here the last thing is the output parser so this will just tell the model to spit out uh the result in uh uh in the string format rather than the big Json object that you get we can also take a look at how the output changes with and without the parser okay so that's very simple for the parser all you do is just instantiate a a uh a string output parser class our object uh okay so at this point we have all these three created now we just need to create our chain okay now creating the chain is I think the most simple uh part of the code so we'll just have our chain we have our template and then we're going to pipe it into the model and then we're going to pipe the output into the parsel that's all you have to do so using the pipe operators you're essentially chaining the different parts of your uh of this llm uh model right so you'll see over here for instance we had uh The Prompt the model the output that is the same as the The Prompt the model and the output parser over here okay so we have prompt model output there's also a way to do it without the pipe operators but uh according to the Lang chain documentation they really want you to use the pipe operators for simple cases like the one we're building right now okay and then finally now that we have the chain created uh the last step remaining is to actually invoke or call the API so to do that we all want to do is just do the chain. invoke and and within invoke you're going to pass in a dictionary where each item is the uh is the values for these Dynamic uh parts of the string okay so you have size and City so we're going to have one uh one key value pair for size so let's say we want it to reply in super short uh super short way okay and then for the city let's say we will start with Atlanta that's where I am okay and finally we're going to print the result all right so quick recap before we run this we have the model which is just one line of code and then we created our prompt from two separate messages one is a system message where we tell open aai uh or in this case open AI but usually just a model how to answer our question uh and then we have the human template message which is going to be just your actual question we create our prompt here and then we have a parser that's going to help uh get the response out of the uh out of the out of the payload that the model is giving us and into a string and then finally we have uh the chain we Define the chain and then we invoke it with whatever whatever arguments that we want to pass in so let's I ran it before as you can see but we're going to run it again here uh so we're in simple proms Pi okay I'll maybe do one level of Zoom okay so you saw that we got uh Georgia Aquarium uh Botanical Garden pedman Park High Museum of Art and the Martin Luther King Historic Site awesome now if you want to maybe change the city to let's say San Francisco and then run it it's going to take a while and now we have Golden Gate Bridge alcatra Island fisherman worth Chinatown and lomber Street okay it can be let's try a different country Tre so let's try London okay so you have five new ones here now for the size uh let's try a different one let's try in uh side in extra details or for given size we're going say super long okay for the same one now you're going to see a much detailed response that's way it's taking much longer because you have to uh process a much larger chunk of chunk of string uh right we'll give it give it a couple of seconds we should get something or at least I hope we should yeah there's a way you you can like tell the model to stream the response and we're going to take a look at that down the road as well uh but yeah you'll see that it took a while but you got essentially a a story for each of the attractions that they mentioned okay all right we're going to maybe try one more which is going to be like medium so this should be be faster than the extra long one that we had and this is one good example why you want to stream the answer from the model to your code to your application cuz that way the user does not have to just stare at a blank screen instead as the model is generating the response in small chunks you can already start showing it to the to the user and that's what you see with all the models out there like Chad GPT per perplexity and all of them and we're going to learn how to do that in one of the next videos as well but yeah you're going to see here that we have the same five but the details are much lower or much less than what we had before but yeah and let's see we'll go back to the diagram um now you'll see that the chain over here is the chain here right now the cool thing is if you wanted to use something like the cloud API or the Gemini API all you have to do is change this line of code okay so instead of like a chat open AI I think it becomes like a chat CLA AI or a chat Gemini Ai and everything else you can keep the same similarly uh let's actually uh look at the prompt right if you were to change the prompt all you have to do is change it here so let's say we don't want the let's say we change the prompt okay let's say we say uh let's say you are a food expert let's say we don't even pass anything here okay so we just tell the system or open AI in this case that you are a food expert you answer everything with nutritional data something like that okay and then let's say we say uh What uh tell me if we're going to say food with some data okay and then for food so now we don't have when we're invoking the model we don't have the size anymore we just have food let's say we'll do a ham okay and that now what we're doing is we're changing the prompt without making any other changes to anywhere else in the in our design okay let's run it and see what we get it should be quicker uh all right we'll give it okay so you see like the response here a typical hamburger uh the the calories and all the details here okay and now if you want to do it for maybe like a broccoli and let's say like uh you keep your answers brief okay that way we're telling the model not to give like a a huge blob of text we're going to run it again yes it just says broccoli is healthy and then a few new uh nutritional facts about it we'll try maybe one more uh let's say uh we're going to do uh I don't know let's say let's say tuna sandwich yeah now you'll see it also calls out the high sodium and high Mercury uh okay so we have seen how you can swap the models around we have seen how you can change your prompt and still keep them Dynamic the last thing is to string output parser so let's say we did not have this okay and then we're going to have our model without the parser and then we're going to invoke it everything the same and I'm going to print the result okay let's see what do we get okay so do you see like uh from the model we get this uh this object essentially uh with like content is what you care about which is until here and then we have like a lot of uh additional keyword uh keyword arguments number of tokens which model was used all the metadata that openai is sending us back and that's why you want to if you don't want to use a parser you could just do result. content that works just as well yeah so that way you get this but again instead of doing this cuz maybe maybe when you're calling uh CLA instead of open a your result won't be or the result you care about won't be in content maybe it will be in data okay uh so if you actually write this code yourself rather than using the parser you have to write additional code to handle the different llm specific logic so instead of that if you're using the parser here so we're going to uncomment it again we're going to add it back instead of you're using the parser you don't care the underlying uh response of the model it'll just give you the result in the string format that you care about okay and do it again okay and there you go you just get the string that you care about all right so hopefully that was helpful in the next video we're going to build more of a chat functionality so instead of making it sort of like one and done like we have here where we're asking the question getting an answer and not asking a follow-up in the next video we're going to actually have a loop where we asked a model of question the model gives a response back and then we uh can keep asking follow-up questions so very similar to what you see with perplexity chat GPT and other other llm uis okay so that's going to be the next video I will uh I will meet youall there then and thank you for watching until the end I will catch youall later bye-bye",
    "summary": "This video demonstrates how to use LangChain to interact with large language models (LLMs) like OpenAI in a modular and flexible way.  LangChain provides tools to easily create prompts, parse outputs, and chain multiple steps together.  The example shows creating a simple chain for getting responses from OpenAI, handling dynamic prompts (system and user messages), and using an output parser. The key benefit is decoupling application code from specific models for easier future updates and improved maintainability.",
    "tweet": "Learn how to build flexible LLM applications with LangChain!  This video shows creating a chain to interact with OpenAI, handling dynamic prompts, and parsing outputs.  Boost your AI projects with modularity and swappable models easily. #LangChain #LLM #OpenAI",
    "userprompt": "most important point",
    "video_title": "Building A Simple Chain (LangChain 101)",
    "video_description": "Let's build a flexible chain that can be used with OpenAI, Claude, or any other LLM you choose. \n\nLangChain Concepts: https://python.langchain.com/docs/concepts/\nLangChain Tutorials: https://python.langchain.com/docs/tutorials/\n\nðŸ‘‹ Say hi to me at: https://irtizahafiz.com\n\n0:00 Benefits of LangChain\n3:35 Starts Coding (Python)\n12:40 Example 1\n16:44 Example 2\n19:00 Output Parser\n21:00 Outro / Next Video Info",
    "video_url": "https://www.youtube.com/watch?v=bX1Q1atain4",
    "created_at": "2024-12-01T11:10:16.626000"
  }